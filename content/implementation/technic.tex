\section{Technische Umsetzung und Struktur} \label{eq:technic}

Wie bereits erwähnt, basiert das Project Tango System auf Googles Android Betriebsystem. Dies ermöglicht es Anwendungen mit bestehenden und bewerten Technologien wie OpenGL, Rajawali\footnote{Android OpenGL ES 2.0/3.0 Java Engine - \url{https://goo.gl/r9Ohdj} (27.02.16)} oder der Unity Engine entwickeln zu können. Project Tango bietet hierfür drei verschiedene Schnittstellen, in C/C++, Java und Unity (Mono Framework in C\#), um auf die Sensordaten in verschiedenen Umgebungen zugreifen zu können. Im laufe dieser Arbeit wurden alle Schnittstellen mit verschiedensten prototypischen Entwicklungen getestet.

Der finale Prototyp wurde letztendlich in C/C++ entwickelt und basiert auf dem Android NDK\footnote{Android Native Development Kit - \url{http://goo.gl/ananZT} (27.02.16)}. Letztendlich greifen die anderen, höher angesiedelten Schnittstellen auf genau die selbe native Implementierung zurück, um sie in Java und Unity zu Verfügung zu stellen. Außerdem ermöglicht die native Entwicklung, neben Performancevorteilen, den vollen Zugriff auf OpenGL Mechanismen, die von Rajawali (der OpenGL Java Abstraktion) gegebenenfalls ausgeschlossen wird. 

Das Project Tango Team stellt für die native Entwicklung die eigentliche Tango Schnittstellen Bibliothek\footnote{Project Tango C API - \url{https://goo.gl/lbBfAp} (27.02.16)}, eine Support-Bibliothek\footnote{Project Tango C Suppport API - \url{https://goo.gl/VGyeKm} (27.02.16)} und eine einfache Kapselung für OpenGL Anwendungen mit dem Namen TangoGL\footnote{TangoGL Repository - \url{https://goo.gl/ymDCsJ} (27.02.16)} an. Die Support-Bibliothek bietet verschiedene Hilfsfunktionen zur Datenverarbeitung und Allokation. TangoGL wiederum erleichtert den Einstieg in die OpenGL Entwicklung und übernimmt die Grundlegende Struktur und Interoperabilität zu Project Tango. Zum Beispiel gibt es Methoden, um Tango Positionsdaten in eine Translationsmatrix umrechnen zu können oder Klassen, die das Rendern der aktuellen RGB Kamera Textur übernehmen. 

Abbildung \ref{fig:structure} zeigt grob den strukturellen Aufbau der Android Applikation. Der obere Teil der Grafik bezieht sich dabei auf den in Java implementierten Teil, der die Nutzeroberfläche, ihre Interaktion und den Renderingcanvas beinhaltet. Die Activity stellt jedoch nur einen kleinen Teil der Anwendung dar, denn alle Interaktionen und Ereignisse werden über ein Java Native Interface (JNI) zum nativen Teil der Anwendung geleitet, welcher die Ansprache der Schnittstellen, die Prozesslogik und das Rendering selbst beinhaltet. 

\begin{figure}[h]
  \centering
	\includegraphics[width=1.0\textwidth]{content/images/implementation/uml.png} 
  \caption{Struktureller Aufbau des Prototypen}
  \label{fig:structure}
\end{figure}

Die Hauptklasse \enquote{ARApp} in der Grafik widmet sich in der Anwendung nur der Anreicherung und Weiterleitung von JNI Informationen und der Ansprache der Project Tango Schnittstelle. Kern der Anwendung ist die \enquote{Scene} Klasse, welche die Sensorinformationen an das entsprechend aktive Verfahren zur Tiefenbild Generierung weiterreicht. So wird zum Beispiel die Pointcloud an das Chisel, Pointcloud oder Plane Drawable weitergereicht, damit Sie ein aktualisiertes Tiefenbild rendern oder die Rekonstruktion anreichern können. Auch das Farbbild der Kamera gelangt über die Scene zum RGB Drawable, welches letztendlich als eine texturierte Fläche am Ende des Kamera Frustum dargestellt wird. Die Szene selbst ermöglicht durch den Einsatz von OpenCV\footnote{Open Source Computer Vision - \url{http://opencv.org/} (26.03.16)} den optionalen Filter Prozess durch den Guided FIlter. Um das zu ermöglichen wird ein OpenGL Framebuffer eingesetzt. Das Listing \ref{lst:rendering} zeigt das Vorgehen beim Rendering der Szene.

\begin{lstlisting}[mathescape,caption=Rendering der Szene, label=lst:rendering, float=htbp]
Eingang: Farbbild $rgb$, Zusatz Tiefenbuffer $tb$
         Haupt Frambuffer $fb$ mit Farb und Tiefenanteil
Ausgang: Resultierendes Hauptbild $fb$

Leere $fb$ und $tb$

Wenn die Pointcloud Projektion aktiv ist
    Rendere die Tiefe der Pointcloud auf $tb$
Wenn die Chisel Rekonstruktion aktiv ist
    Rendere die Tiefe der Chisel Rekonstruktion  auf $tb$
Wenn die Ebenen Rekonstruktion aktiv ist
    Rendere die Tiefe der Ebenen Rekonstruktion auf $tb$
    
Wenn gefiltert werden soll
    Wende den GuidedFilter auf $tb$ mit $rgb$ als Guide an
        
Rendere $rgb$ auf $fb$ 
ersetzte den Tiefenbuffer von $fb$ druch $tb$
aktiviere den OpenGL Depth Test
rendere das 3D Modell auf $fb$
\end{lstlisting}
